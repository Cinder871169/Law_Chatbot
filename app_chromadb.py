import os
import sys
import streamlit as st
from dotenv import load_dotenv
from langchain_google_genai import GoogleGenerativeAIEmbeddings, ChatGoogleGenerativeAI
from langchain_chroma import Chroma
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate

# --- C·∫§U H√åNH GIAO DI·ªÜN ---
st.set_page_config(page_title="Tr·ª£ l√Ω Ph√°p l√Ω AI", page_icon="‚öñÔ∏è")
st.title("‚öñÔ∏è Tr·ª£ l√Ω tra c·ª©u Ph√°p lu·∫≠t")

with st.sidebar:
    st.title("‚öôÔ∏è T√πy ch·ªçn")
    if st.button("üóëÔ∏è X√≥a tin nh·∫Øn tr√™n m√†n h√¨nh"):
        st.session_state.messages = [
            {
                "role": "assistant",
                "content": "Xin ch√†o! T√¥i c√≥ th·ªÉ gi√∫p b·∫°n tra c·ª©u th√¥ng tin ph√°p lu·∫≠t g√¨ h√¥m nay?",
            }
        ]
        st.success("ƒê√£ d·ªçn d·∫πp m√†n h√¨nh!")

# --- ƒê·ªåC API KEY ---
load_dotenv(override=True)
DB_CHROMA_PATH = "chroma_db_luat"

# Ki·ªÉm tra Database
if not os.path.exists(DB_CHROMA_PATH):
    st.error("Ch∆∞a c√≥ d·ªØ li·ªáu lu·∫≠t!")
    st.stop()

# --- KH·ªûI T·∫†O M·∫†NG V√Ä T√åM KI·∫æM ---
# K·∫øt n·ªëi v√†o DB c√≥ s·∫µn
embeddings = GoogleGenerativeAIEmbeddings(model="models/gemini-embedding-001")
vectorstore = Chroma(persist_directory=DB_CHROMA_PATH, embedding_function=embeddings)
retriever = vectorstore.as_retriever(search_kwargs={"k": 2})
llm = ChatGoogleGenerativeAI(model="gemini-2.5-flash", temperature=0)

# --- X√ÇY D·ª∞NG CHU·ªñI RAG ---
system_prompt = (
    "B·∫°n l√† m·ªôt Lu·∫≠t s∆∞ ·∫£o chuy√™n nghi·ªáp, t·∫≠n t√¢m v√† ch√≠nh x√°c. "
    "Nhi·ªám v·ª• c·ªßa b·∫°n l√† s·ª≠ d·ª•ng c√°c ƒëo·∫°n vƒÉn b·∫£n lu·∫≠t ƒë∆∞·ª£c cung c·∫•p d∆∞·ªõi ƒë√¢y ƒë·ªÉ gi·∫£i ƒë√°p th·∫Øc m·∫Øc c·ªßa ng∆∞·ªùi d√πng.\n\n"
    "QUY T·∫ÆC TR·∫¢ L·ªúI:\n"
    "1. T√≠nh ch√≠nh x√°c: Ch·ªâ tr·∫£ l·ªùi d·ª±a tr√™n n·ªôi dung c√≥ trong 'Ng·ªØ c·∫£nh lu·∫≠t ph√°p'. "
    "N·∫øu th√¥ng tin kh√¥ng c√≥, h√£y l·ªãch s·ª± tr·∫£ l·ªùi: 'R·∫•t ti·∫øc, t√¥i kh√¥ng t√¨m th·∫•y quy ƒë·ªãnh n√†y trong c√°c vƒÉn b·∫£n lu·∫≠t hi·ªán c√≥ trong h·ªá th·ªëng.'\n"
    "2. Tr√≠ch d·∫´n ngu·ªìn: M·ªói c√¢u tr·∫£ l·ªùi B·∫ÆT BU·ªòC ph·∫£i k√®m theo t√™n Lu·∫≠t, s·ªë ƒêi·ªÅu v√† s·ªë Kho·∫£n c·ª• th·ªÉ (v√≠ d·ª•: Theo ƒêi·ªÅu 8, Lu·∫≠t H√¥n nh√¢n v√† Gia ƒë√¨nh 2014).\n"
    "3. C·∫•u tr√∫c: S·ª≠ d·ª•ng g·∫°ch ƒë·∫ßu d√≤ng cho c√°c danh s√°ch ƒëi·ªÅu ki·ªán ho·∫∑c th·ªß t·ª•c ƒë·ªÉ ng∆∞·ªùi d√πng d·ªÖ theo d√µi.\n"
    "4. Tuy·ªát ƒë·ªëi kh√¥ng t·ª± b·ªãa ra c√°c con s·ªë ho·∫∑c th·ªùi h·∫°n ph√°p l√Ω n·∫øu kh√¥ng th·∫•y trong vƒÉn b·∫£n.\n\n"
    "Ng·ªØ c·∫£nh lu·∫≠t ph√°p:\n{context}"
)

qa_prompt = ChatPromptTemplate.from_messages(
    [
        ("system", system_prompt),
        ("human", "{input}"),
    ]
)


def format_docs(docs):
    return "\n\n".join(doc.page_content for doc in docs)


rag_chain = (
    {"context": retriever | format_docs, "input": RunnablePassthrough()}
    | qa_prompt
    | llm
    | StrOutputParser()
)

# --- HI·ªÇN TH·ªä GIAO DI·ªÜN CHAT ---
if "messages" not in st.session_state:
    st.session_state.messages = [
        {
            "role": "assistant",
            "content": "Xin ch√†o! T√¥i c√≥ th·ªÉ gi√∫p b·∫°n tra c·ª©u th√¥ng tin ph√°p lu·∫≠t g√¨ h√¥m nay?",
        }
    ]

for msg in st.session_state.messages:
    st.chat_message(msg["role"]).write(msg["content"])

if user_query := st.chat_input("Nh·∫≠p c√¢u h·ªèi..."):
    st.session_state.messages.append({"role": "user", "content": user_query})
    st.chat_message("user").write(user_query)

    with st.chat_message("assistant"):
        with st.spinner("ƒêang tra c·ª©u lu·∫≠t..."):
            try:
                response = rag_chain.invoke(user_query)
                st.write(response)
                st.session_state.messages.append(
                    {"role": "assistant", "content": response}
                )
            except Exception as e:
                st.error(f"L·ªói truy xu·∫•t: {e}")
